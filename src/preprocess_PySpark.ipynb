{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3d4efb",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534cb04",
   "metadata": {},
   "source": [
    "Tras probar con Polars y su LazyFrame, pese a la optimizacion de memoria y de rendimiento que ofrece esta libreria, mi hardware, no permite operaciones con datos de un tamaño tan grande como el de este dataset.\n",
    "\n",
    "Al analizar mis opciones, he decidido usar PySpark, que utiliza operaciones vectorizadas y columnares, para poder trabajar con estas dimensiones de datos. Además utiliza SQL, un lenguaje con el que tengo experiencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfed03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d936df",
   "metadata": {},
   "source": [
    "Como ya he hecho un analisis de los datos de cada tabla y como unirlos en polars, en este script unicamente hare el pipeline de union para tener un dataset limpio para comenzar mi EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73850065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = (BASE_DIR/'data').resolve()\n",
    "RAW_DIR = (DATA_DIR/'raw').resolve()\n",
    "PROCESSED_DIR = (DATA_DIR/'processed').resolve()\n",
    "\n",
    "CUTOFF_DATE = date(2017, 2, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52ea4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName('KKBox') \\\n",
    "    .master('local[*]')\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c46661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members = spark.read.csv(str(RAW_DIR/'members_v3.csv'), header=True, inferSchema=True)\n",
    "df_train = spark.read.csv(str(RAW_DIR/'train.csv'), header=True, inferSchema=True)\n",
    "df_tx = spark.read.csv(str(RAW_DIR/'transactions.csv'), header=True, inferSchema=True)\n",
    "df_user_logs = spark.read.csv(str(RAW_DIR/'user_logs.csv'), header=True, inferSchema=True).repartition(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569ecd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members = df_members.drop('registered_via')\n",
    "\n",
    "df_members = df_members.withColumn(\n",
    "    \"registration_init_time\",\n",
    "    sf.to_date(sf.col(\"registration_init_time\").cast(\"string\"), \"yyyyMMdd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d781cb7",
   "metadata": {},
   "source": [
    "### Members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8781d6",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257be621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx = df_tx.drop('payment_method_id')\n",
    "df_tx = df_tx.withColumn(\n",
    "        'transaction_date', sf.to_date(sf.col('transaction_date'), 'yyyyMMdd')\n",
    "    ).withColumn(\n",
    "        'membership_expire_date', sf.to_date(sf.col('membership_expire_date'), 'yyyyMMdd')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2606aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_agg = (\n",
    "  df_tx\n",
    "  .filter(sf.col(\"transaction_date\") <= sf.lit(CUTOFF_DATE))\n",
    "  .groupBy(\"msno\")\n",
    "  .agg(\n",
    "      sf.count(\"*\").alias(\"num_transactions\"),\n",
    "      sf.sum(\"actual_amount_paid\").alias(\"total_amount_paid\"),\n",
    "      sf.avg(\"is_auto_renew\").alias(\"autorenew_rate\"),\n",
    "      sf.avg(\"is_cancel\").alias(\"cancel_rate\"),\n",
    "      sf.greatest(sf.datediff(sf.max(\"membership_expire_date\"), sf.max(\"transaction_date\")), sf.lit(0)).alias(\"days_until_expire\"),\n",
    "      sf.max_by(sf.col(\"payment_plan_days\"), sf.col(\"transaction_date\")).alias(\"plan_days_last\"),\n",
    "      sf.max_by(sf.col(\"plan_list_price\")  , sf.col(\"transaction_date\")).alias(\"plan_price_last\"),\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40888",
   "metadata": {},
   "source": [
    "### User_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db890122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_logs = df_user_logs.withColumn(\n",
    "    'date', sf.to_date(sf.col('date'), 'yyyyMMdd')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91821d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = (\n",
    "    df_user_logs\n",
    "    .filter(sf.col(\"date\") <= sf.lit(CUTOFF_DATE))\n",
    "    .withColumn(\"total_secs\", sf.greatest(sf.col(\"total_secs\"), sf.lit(0.0)))\n",
    ")\n",
    "\n",
    "logs_agg = (\n",
    "    logs\n",
    "    .groupBy(\"msno\")\n",
    "    .agg(\n",
    "        sf.countDistinct(\"date\").alias(\"active_days\"),\n",
    "        sf.sum(\"num_25\").alias(\"total_25\"),\n",
    "        sf.sum(\"num_50\").alias(\"total_50\"),\n",
    "        sf.sum(\"num_75\").alias(\"total_75\"),\n",
    "        sf.sum(\"num_985\").alias(\"total_985\"),\n",
    "        sf.sum(\"num_100\").alias(\"total_100\"),\n",
    "        sf.sum(\"num_unq\").alias(\"unique_song_listened\"),\n",
    "        sf.max(\"num_unq\").alias(\"max_unique_per_day\"),\n",
    "        sf.sum(\"total_secs\").alias(\"total_sec_listened\"),\n",
    "        sf.min(\"date\").alias(\"first_activity\"),\n",
    "        sf.max(\"date\").alias(\"last_activity\"),\n",
    "    )\n",
    "    .withColumn(\"lifetime_days\", sf.datediff(\"last_activity\", \"first_activity\"))\n",
    "    .withColumn(\n",
    "        \"avg_sec_active_day\",\n",
    "        sf.when(sf.col(\"active_days\") > 0,\n",
    "                sf.col(\"total_sec_listened\") / sf.col(\"active_days\"))\n",
    "          .otherwise(sf.lit(0.0))\n",
    "    )\n",
    "    # clip físico [0, 86400]\n",
    "    .withColumn(\n",
    "        \"avg_sec_active_day\",\n",
    "        sf.least(sf.greatest(sf.col(\"avg_sec_active_day\"), sf.lit(0.0)), sf.lit(86400.0))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7265ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = df_members.select(\"msno\",\"gender\",\"city\",\"bd\",\"registration_init_time\")\n",
    "final = (df_train\n",
    "         .join(members, \"msno\", \"left\")\n",
    "         .join(logs_agg, \"msno\", \"left\")\n",
    "         .join(tx_agg, \"msno\", \"left\"))\n",
    "\n",
    "final.to_parquet(str(PROCESSED_DIR / \"data_processed.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geeks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
